{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kapipy","text":"<p>A python client for accessing and querying datasets from geospatial open data portals such as LINZ, Stats NZ and LRIS.</p>"},{"location":"#overview","title":"Overview","text":"<p>kapipy is a Python package that provides a python interface to the Koordinates geospatial content management system. It allows users to connect to a data portal, retrieve metadata, and query vector layers and tables. </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install kapipy\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<ul> <li>Import kapipy.  </li> <li>Create a GISK object, passing in an api key.  </li> <li>Get a reference to an item using {gis}.content.get({layer_id})</li> <li>Perform actions on the item.  </li> </ul> <p>Basic example:  </p> <pre><code>from kapipy.gis import GISK\nlinz = GISK(name=\"linz\", api_key=\"my-linz-api-key\")\nrail_station_layer_id = \"50318\"\nitm = linz.content.get(rail_station_layer_id)\ndata = itm.query()\ndata.df.head()\n</code></pre>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>Kapipy is provided as-is. The author has no affiliation with either Koordinates nor LINZ, Stats NZ or LRIS. As such, the underlying API's and services may change at any time without warning and break these modules.  </p> <p>This project does not cover the full spectrum of the Koordinates API and probably never will. It focuses currently on basic workflows such as connecting using an api key, getting references to datasets and downloading them.  </p> <p>Suggestions and bug reports can be made by submitting issues via the GitHub page.  </p>"},{"location":"about/","title":"How this came about","text":"<p>In New Zealand, where I am based, we are extremely lucky to have a wide range of open geospatial data available to us. One main source of this data is LINZ who provide an open data website with a wide range of data, including, for example, the cadastral datasets. Their website has a great UI for exploring and exporting data, as well as instructions for loading web services directly into desktop applications.  </p> <p>I work a lot with local councils and other organisations wanting to use data from LINZ within ETL processes using python. Over the years, I have seen (and written!) various implementations of python scripts that query the LINZ data API's and download data. Usually these are some variation of sending requests to the WFS endpoint. A common use case is to download changeset data to keep local data up to date. The local data is then mashed up with internal datasets, such as rating information, to create various derived outputs.  </p> <p>My role as a GIS Advisor is primarily within the ArcGIS eco-system, and as such I work a lot with the ArcGIS API for Python. This is a comprehensive python package provided by Esri to interact with the ArcGIS data portals: ArcGIS Online and ArcGIS Enterprise.  </p> <p>I wrote and re-wrote several LINZ export helper scripts, and eventually I realised that I wanted to provide a familiar experience for extracting data from LINZ, with similarities to how the ArcGIS API for Python package allows to browse ArcGIS portal content.  </p> <p>The general approach is: - Get a reference to a portal. - Get a reference to a content item from that portal. - Perform actions on that content.  </p> <p>To me, this feels like a natural way to interact with a data portal. Kapipy wraps the Koordinates export API to provide the data as a zip file download, and the WFS API to return either a geopandas GeoDataFrame or an ArcGIS Spatially Enabled DataFrame. This makes it useful to all users, but extra convenient for ArcGIS users who may already be using sdf inside scripts. In fact, I find myself using kapipy within Jupyter and ArcGIS Notebooks as I can now bring in and display data layers from both LINZ and ArcGIS portals using a similar approach.   </p> <p>As a bonus for me, this was the first time I had attempted building a python package and publishing to PyPi. Constructing a package like this is a great way to learn, as it forces you to think about software architecture. Already I have re-organised the modules and components several times!</p> <p>For example, after the AI overlords helped me implement a JobResult class, I initially kept a jobs list attribute with the item class and added the job object to that list. I thought that would enable the user to retrospectively review and redownload prior jobs. But then I changed my mind. How often would a user create multiple download jobs for the same item within the same run of a script? There seemed little utility for the job list. Then it occurred to me that keeping a more global jobs list with the ContentManager might be more useful. If the ContentManager controlled the exports and kept a list of them, I could envisage a workflow where a script generated export requests for several items, then one call to the ContentManager download method could download them all to a folder in one go.  </p> <p>Your use cases may differ. Feel free to contribute ideas, feedback and code via the GitHub repo.  </p> <pre><code>*|********\n*|*/******\n*|/**|\\***\n*|\\**|*\\**\n*|*\\*|*/**\n*****|/***\n*****|****\n*****|****\n</code></pre>"},{"location":"development_notes/","title":"Development Notes","text":"<p>These are just general notes for the author to help remember design choices, rabbit holes and how they panned out, etc.  </p>"},{"location":"development_notes/#installation","title":"Installation","text":"<p>When I run <code>uv sync</code> and <code>uv pip install -e .</code> on a new cloned copy, if I have a python environment activated already in my terminal it seems to do odd things sometimes. VS Code sometimes activates automatically depending on settings. And sometimes those settings vary between PowerShell and the standard Command Prompt. So I find it best to ensure I open a separate Command Prompt window with nothing activated, run those initial commands there, and then open up VS Code and any terminal windows. </p> <p>If installing from the whl file using UV, remember to add the package name.</p> <pre><code>uv pip install kapipy@path/to/packagefile.whl\n</code></pre> <p>If using the code directly rather than installing from PyPi (once it is uploaded to there), run the following to install the package locally in editable mode.</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"development_notes/#arcgis","title":"ArcGIS","text":"<p>Recommend using the existing conda manager that comes installed with ArcGIS Pro or ArcGIS Server. If using arcgis module but not arcpy installed, need to install: - pyproj - shapely - pyshp</p> <p>Optionally pip install dotenv if wanting to use that. If creating a blank conda environment (rather than cloning the default), and wanting to use Jupyter notebooks, install: - ipykernel - ptyprocess - comm</p> <p>When converting geojson to sdf, I first tried using FeatureSet.from_geojson(geojson) but I found that it assumes that the geojson is in wgs84 (which is the strict definition of geojson). But in our case sometimes we often get the geojson returned already in NZTM/2193. So the from_geojson ended up with incorrect spatial data.  So I write a function that manually converts the geojson to an ArcGIS FeatureSet by passing in the json, fields and wkid. Then I can convert that FeatureSet to an sdf.  </p>"},{"location":"development_notes/#development-using-jupyter-notebooks","title":"Development using Jupyter Notebooks","text":"<p>I got the following tips from Cookie Cutter Data Science. Cookie Cutter Data Science </p> <p>Make the project a python package and install it locally. I was using UV, so I ran this command:</p> <pre><code>uv pip install -e .  \n</code></pre> <p>Then, in my notebook, I included this cell first:  </p> <pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>This allowed me to load my local python package like this:  </p> <pre><code>from k_data_helpers import KServer  \n</code></pre> <p>And it would hot reload any changes I made while developing.</p>"},{"location":"development_notes/#koordinates-api","title":"Koordinates API","text":""},{"location":"development_notes/#export-api","title":"Export API","text":"<p>The export api doesn't appear to have an option for applying a cql_filter or any similar filter. Only extent. The extent appears to have to be a geojson geometry object. Note that this is just the geometry part, not the properties or collection. And it would have to be in WGS84. I might look at ways to handle passing in geometry objects of different types, such as from a geopandas geometry, and behind the scenes just handling that and converting to the corrrect format.  </p> <p>Also, the export API treats the extent as a crop, and so features will be clipped. This may not be desired in all situations, e.g. clipping Property Parcels is not usually a good thing as someone may inadvertantly think that that is the actually parcel geometry, not realising it was clipped. The question is: how to handle this? Just warn the user in documentation and leave it up to them? Apply a buffer and do some post-processing? I'm inclined to do less, let the system supply as it is designed, and educate the user. This does imply the end user needs to do a little bit extra work but I would rather the user explicitly get the output and the module logic not get in the way.  </p> <p>It does appear to allow generating an export of multiple items at once. E.g. you could request several layers in one zipped file geodatabase. Currently, this wrapper only supports one at a time, because I didn't realise at the time you could do multiple, so this would be a good enhancement for the future. The current approach is based off starting with an item and downloading that. So a multi item download would need to be initiated by a higher order class, perhaps the ContentManager?  </p> <p>Need to think about how a user would most likely pass in the parameters for a multi download without constructing the whole list verbosely, but allowing them to do that if they wish.  </p>"},{"location":"development_notes/#notes-on-design-choices","title":"Notes on design choices","text":""},{"location":"development_notes/#owslib","title":"OWSLib","text":"<p>I investigated using the OWSLib python package to download the WFS data, but discovered that it doesn't support the CQL filter keyword option that the LINZ GeoServer provides. OGC filters were still an option, but seem very complex to construct and I believe most users would prefer to use the simpler CQL which is more similar to SQL. So I moved back to using a basic request to the WFS endpoint. The OWSLib package would provide more scope for expansion, but since the intent of this helper library is primarily focused on Koordinates and LINZ in particular, we can afford to be a little more opinionated on our approach, such as not having to support all the WFS versions. I'm not sure if the LINZ WFS endpoint is strictly equivalent with all other Koordinates WFS endpoints. So the implementation at the moment is coded to work with LINZ and might not work in other places.  </p>"},{"location":"development_notes/#prompts","title":"Prompts","text":"<p>Can you review the docstrings for all classes, methods and functions. Make sure they are accurate and reflect the correct Parameters and Return values. Ensure the syntax is correct, use the word Parameters instead of Args, and ensure the formatting is consistent for use with MkDocs and the Google format. Only provide docstrings that actually need to change. Provide each docstring in a separate section so I can copy and paste it. There is no need to provide the original for comparison. Provide an update for str and repr if necessary. Check that the type hints are accurate.</p>"},{"location":"development_notes/#tests","title":"Tests","text":"<p>Tests are written using pytest.</p> <p>To run all tests with logging. Leave off the log parameter if not wanting logging.  </p> <pre><code>uv run -m pytest --log-cli-level=INFO\n</code></pre> <p>To run a specific test, replace the relevant file name and test function.  </p> <pre><code>uv run -m pytest tests/test_simple.py::test_validate_layer_export_params --log-cli-level=INFO\n</code></pre> <p>There is currently very limited test coverage. Any live tests require a \"LINZ_API_KEY\" entry to exist in a .env file in the root project folder.  </p> <p>To manually test the current build in a conda ArcGIS environment, need to manually install the current build into that environment. - Open the ArcGIS python command prompt. - Activate the desired environment. - Change directory into the local kapipy development folder. - If necessary, uninstall any existing install of kapipy. - Run the following pip command.  </p> <pre><code>pip install -e .\n</code></pre>"},{"location":"development_notes/#build-guidelines-and-processes","title":"Build: Guidelines and Processes","text":"<ul> <li>Do all development work on the develop branch.  </li> <li>Push all commits to Github on develop.  </li> <li>When ready to create a release:</li> <li> <ul> <li>Increment the version in pyproject.toml and version.py  </li> </ul> </li> <li> <ul> <li>Create a pull request in Github across to the main branch.</li> </ul> </li> <li> <ul> <li>Create a release in main.  </li> </ul> </li> <li>This will trigger Github action which will build and publish to PyPi.  </li> </ul> <p>Pushing commits to develop branch in Github should trigger Github action to run pytest tests.  </p> <p>Uv commands to manually build and publish.  </p> <pre><code>uv build --no-sources\n\n# testpypi\nuv publish --index testpypi\n\n# or PyPi\nuv publish --index pypi\n</code></pre>"},{"location":"development_notes/#documentation","title":"Documentation","text":"<p>Documentation uses MkDocs to generate and publish documentation website using Github Pages.  </p> <p>Locally, run <code>uv mkdocs build</code>  then <code>uv mkdocs gh-deploy --clean</code> </p> <p>This could probably be set up as a Github action. If the gh-deploy was run from a Github workflow, need to figure out a way to use secrets.GITHUB_TOKEN. Or does it need authentication at all? Maybe it just works from there?    </p>"},{"location":"development_notes/#enhancements","title":"Enhancements","text":"<p>Should there be a max days for the changesets? Or would that be up to the user to enforce? I'm leaning towards that being up to the user.  </p> <p>Is there a way to obtain from the API ahead of time what the file geodatabase and feature class names will be?  </p>"},{"location":"examples/","title":"Example scripts","text":""},{"location":"examples/#keep-a-copy-of-nz-primary-parcels-for-your-district-up-to-date","title":"Keep a copy of NZ Primary Parcels for your district up to date","text":"<p>A common workflow for NZ district councils is to download a copy of the NZ Primary Parcels for their district, and then keep it up to date with changesets. The approach here is split into two parts: - A manual step to download the initial data and prepare it as the target for the next step. - An step that downloads and applies changesets, intended to be scheduled and run automatically.  </p>"},{"location":"examples/#download-data-for-district","title":"Download data for district","text":"<pre><code>from kapipy.gis import GISK  \nimport zipfile\nimport pandas as pd\nfrom arcgis.features import GeoAccessor, GeoSeriesAccessor\n\n# Connect to LINZ   \nlinz = GISK(name=\"linz\", api_key=api_key)\n\n# Load in extent shape file  \ndistrict_extent = r\"c:/data/matamata_piako.shp\"\ndistrict_extent_sdf = pd.DataFrame.spatial.from_featureclass(district_extent)\n\n# Download the primary parcels data  \nnz_primary_parcels_layer_id = \"50772\"  \nitm = linz.content.get(nz_primary_parcels_layer_id)\njob = itm.export(\n    export_format=\"geodatabase\",\n    out_sr=2193,\n    extent=district_extent_sdf,\n    )\nresult = job.download(folder=r\"c:/data/linz\")\n\nwith zipfile.ZipFile(result.file_path, 'r') as zip_ref:\n    zip_ref.extractall()\n</code></pre> <p>todo: in above example, how to delete features that don't intersect the extent?  how to get the name of the feature class? Can we use result.filename as the feature class name? </p> <p>todo: example of how to download and apply changeset.  </p> <pre><code>#todo.....\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#is-this-a-comprehensive-python-wrapper-for-the-koordinates-api","title":"Is this a comprehensive python wrapper for the Koordinates API?","text":"<p>No. The focus here is primarily on extracting data easily, and it is unlikely to ever move beyond data exploration and export. The project is young, and even within the that narrow scope there is more can be done. At the time of writing, only Vector and Table datasets are implemented, as it felt like those are the core datasets that might require regular downloading.</p>"},{"location":"faq/#how-do-i-get-an-api-key","title":"How do I get an API key?","text":"<p>LINZ, Stats NZ and LRIS all have sign up pages. They should be a common Koordinates login, but you need to generate separate API keys from each site separately. Remember to manually grant the necessary scopes to the api key to perform the various functions.  </p>"},{"location":"faq/#how-do-i-report-bugs-or-provide-feedback","title":"How do I report bugs or provide feedback?","text":"<p>The recommended way is via the GitHub issues page.  .  </p>"},{"location":"faq/#will-it-work-with-other-koordinates-data-portals","title":"Will it work with other Koordinates data portals?","text":"<p>Probably? Maybe? Not sure. Try it and provide feedback if it doesn't! The focus during development is purely on LINZ, Stats NZ and LRIS, so those are the only ones that have been tested.  </p>"},{"location":"faq/#can-i-download-the-entire-nz-primary-parcels-layer-using-the-query-method","title":"Can I download the entire NZ Primary Parcels layer using the query method?","text":"<p>Never tried. And you probably shouldn't either.  For context, the NZ Primary Parcels layer is approx 2.7M polygons. The query method uses the WFS endpoint, and would have to make over 250 large requests to download the data. That seems to be just asking for a network or connection error of some sort to cut you off halfway through. Instead, use the export method to generate and download large datasets, and then use the changeset method to retrieve small changesets and apply those.  </p> <p>(Extra info: Buried in the kapipy code are a couple of variables: DEFAULT_FEATURES_PER_PAGE (10000) and MAX_PAGE_FETCHES (50). So by default kapipy will cut out at 500,000 features. That's a soft limit and you can override the number of features per page, but spare a thought for the servers and perhaps if you're downloading that much, use the export API instead.)  </p>"},{"location":"reference/","title":"API Reference","text":""},{"location":"reference/#kapipy.gis.GISK","title":"GISK","text":"<pre><code>GISK(\n    name=None,\n    url=None,\n    api_key=None,\n    api_version=DEFAULT_API_VERSION,\n)\n</code></pre> <p>Client for connecting to a Koordinates server.</p> <p>Provides methods for authenticating, accessing content, and making HTTP requests to the Koordinates API. Used as the main entry point for interacting with Koordinates-hosted data.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the Koordinates portal. If this is provided, the url is ignored.  </p> </li> <li> <code>url</code>               (<code>str</code>)           \u2013            <p>The base URL of the Koordinates server.</p> </li> <li> <code>_api_version</code>               (<code>str</code>)           \u2013            <p>The API version to use.</p> </li> <li> <code>_content_manager</code>               (<code>ContentManager or None</code>)           \u2013            <p>Cached ContentManager instance.</p> </li> <li> <code>_api_key</code>               (<code>str</code>)           \u2013            <p>The API key for authenticating requests.</p> </li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the portal name is not recognized or if api_key is not provided.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get</code>             \u2013              <p>Makes a synchronous GET request to the specified URL with the provided parameters.</p> </li> <li> <code>reset</code>             \u2013              <p>Resets the GISK instance. </p> </li> </ul>"},{"location":"reference/#kapipy.gis.GISK(name)","title":"<code>name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the Koordinates portal (e.g., 'linz'). If provided, overrides url.</p>"},{"location":"reference/#kapipy.gis.GISK(url)","title":"<code>url</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The base URL of the Koordinates server. Used if name is not provided.</p>"},{"location":"reference/#kapipy.gis.GISK(api_key)","title":"<code>api_key</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The API key for authenticating with the Koordinates server.</p>"},{"location":"reference/#kapipy.gis.GISK(api_version)","title":"<code>api_version</code>","text":"(<code>str</code>, default:                   <code>DEFAULT_API_VERSION</code> )           \u2013            <p>The API version to use. Defaults to 'v1.x'.</p>"},{"location":"reference/#kapipy.gis.GISK.audit","title":"audit  <code>property</code>","text":"<pre><code>audit: AuditManager\n</code></pre> <p>Returns the AuditManager instance for this GISK.</p> <p>Returns:</p> <ul> <li> <code>AuditManager</code> (              <code>AuditManager</code> )          \u2013            <p>The audit manager associated with this GISK.</p> </li> </ul>"},{"location":"reference/#kapipy.gis.GISK.content","title":"content  <code>property</code>","text":"<pre><code>content: ContentManager\n</code></pre> <p>Returns the ContentManager instance for this server.</p> <p>Returns:</p> <ul> <li> <code>ContentManager</code> (              <code>ContentManager</code> )          \u2013            <p>The content manager associated with this server.</p> </li> </ul>"},{"location":"reference/#kapipy.gis.GISK.get","title":"get","text":"<pre><code>get(url: str, params: dict = None) -&gt; dict\n</code></pre> <p>Makes a synchronous GET request to the specified URL with the provided parameters. Injects the API key into the request headers.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The JSON-decoded response from the server.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BadRequest</code>             \u2013            <p>If the request fails with a 400 status code.</p> </li> <li> <code>ServerError</code>             \u2013            <p>For other HTTP errors or request exceptions.</p> </li> </ul>"},{"location":"reference/#kapipy.gis.GISK.get(url)","title":"<code>url</code>","text":"(<code>str</code>)           \u2013            <p>The URL to send the GET request to.</p>"},{"location":"reference/#kapipy.gis.GISK.get(params)","title":"<code>params</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>Query parameters to include in the request. Defaults to None.</p>"},{"location":"reference/#kapipy.gis.GISK.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Resets the GISK instance.  This is useful if the API key or other configurations change.</p> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul>"},{"location":"reference/#kapipy.content_manager.ContentManager","title":"ContentManager","text":"<pre><code>ContentManager(\n    session: SessionManager, audit: AuditManager\n)\n</code></pre> <p>Manages content for a GISK instance.</p> <p>Provides methods to search for, retrieve, and instantiate Koordinates items (layers, tables, etc.) based on their IDs or URLs.</p> <p>Attributes:</p> <ul> <li> <code>jobs</code>               (<code>list</code>)           \u2013            <p>Export jobs list.</p> </li> <li> <code>download_folder</code>               (<code>str</code>)           \u2013            <p>Default folder for downloads.</p> </li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> </ul> <p>Methods:</p> <ul> <li> <code>download</code>             \u2013              <p>Downloads all exports from a list of jobs.</p> </li> <li> <code>get</code>             \u2013              <p>Retrieves and instantiates a content item by ID from the GISK.</p> </li> </ul>"},{"location":"reference/#kapipy.content_manager.ContentManager(session)","title":"<code>session</code>","text":"(<code>SessionManager</code>)           \u2013            <p>The GISK SessionManager.</p>"},{"location":"reference/#kapipy.content_manager.ContentManager(audit)","title":"<code>audit</code>","text":"(<code>AuditManager</code>)           \u2013            <p>The GISK AuditManager.</p>"},{"location":"reference/#kapipy.content_manager.ContentManager.download","title":"download","text":"<pre><code>download(\n    jobs: list[JobResults] = None,\n    folder: str = None,\n    poll_interval: int = 10,\n    force_all: bool = False,\n) -&gt; list[JobResults]\n</code></pre> <p>Downloads all exports from a list of jobs. Polls the jobs until they are finished. As soon as it encounters a finished job, it pauses polling and downloads that file, then resumes polling the remainder.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[JobResults]</code>           \u2013            <p>list[JobResult]: The list of job result objects after download.</p> </li> </ul>"},{"location":"reference/#kapipy.content_manager.ContentManager.download(jobs)","title":"<code>jobs</code>","text":"(<code>list[JobResult]</code>, default:                   <code>None</code> )           \u2013            <p>The list of job result objects to download.</p>"},{"location":"reference/#kapipy.content_manager.ContentManager.download(folder)","title":"<code>folder</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The output folder where files will be saved.</p>"},{"location":"reference/#kapipy.content_manager.ContentManager.download(poll_interval)","title":"<code>poll_interval</code>","text":"(<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The interval in seconds to poll the jobs. Default is 10.</p>"},{"location":"reference/#kapipy.content_manager.ContentManager.get","title":"get","text":"<pre><code>get(id: str) -&gt; dict\n</code></pre> <p>Retrieves and instantiates a content item by ID from the GISK.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>VectorItem or TableItem or None: The instantiated item, depending on its kind, or None if not found.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>BadRequest</code>             \u2013            <p>If the content is not found or the request is invalid.</p> </li> <li> <code>UnknownItemTypeError</code>             \u2013            <p>If the item kind is not supported.</p> </li> <li> <code>ServerError</code>             \u2013            <p>If the item does not have a URL.</p> </li> </ul>"},{"location":"reference/#kapipy.content_manager.ContentManager.get(id)","title":"<code>id</code>","text":"(<code>str</code>)           \u2013            <p>The ID of the content to retrieve.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager","title":"AuditManager","text":"<pre><code>AuditManager()\n</code></pre> <p>Manages auditing for a GISK instance.</p> <p>Provides methods to record and retrieve information relating to interactions with the GISK. All data is stored in a SQLite database.</p> <p>Methods:</p> <ul> <li> <code>add_request_record</code>             \u2013              <p>Adds a request record to the audit database.</p> </li> <li> <code>disable_auditing</code>             \u2013              <p>Disables auditing by setting the enabled flag to False.</p> </li> <li> <code>enable_auditing</code>             \u2013              <p>Enable auditing and create the audit database if it does not exist.</p> </li> <li> <code>get_latest_request_for_item</code>             \u2013              <p>Returns the most recent audit record for the given item_id, optionally filtered by request_type,</p> </li> <li> <code>save_data</code>             \u2013              <p>Save the audit data to a local JSON file.</p> </li> </ul>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record","title":"add_request_record","text":"<pre><code>add_request_record(\n    item_id: int,\n    item_kind: str,\n    item_type: str,\n    request_type: str,\n    request_url: str,\n    request_method: str,\n    request_time: datetime,\n    request_headers: dict,\n    request_params: dict,\n    response: dict = None,\n) -&gt; None\n</code></pre> <p>Adds a request record to the audit database.</p> <p>Converts request_headers and request_params dicts to JSON strings for storage.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(item_id)","title":"<code>item_id</code>","text":"(<code>int</code>)           \u2013            <p>The ID of the item involved in the request.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(item_kind)","title":"<code>item_kind</code>","text":"(<code>str</code>)           \u2013            <p>The kind of the item (e.g., 'vector', 'table').</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(item_type)","title":"<code>item_type</code>","text":"(<code>str</code>)           \u2013            <p>The type of the item.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(request_type)","title":"<code>request_type</code>","text":"(<code>str</code>)           \u2013            <p>The type of request (e.g., 'GET', 'POST').</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(request_url)","title":"<code>request_url</code>","text":"(<code>str</code>)           \u2013            <p>The URL of the request.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(request_method)","title":"<code>request_method</code>","text":"(<code>str</code>)           \u2013            <p>The HTTP method used for the request.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(request_time)","title":"<code>request_time</code>","text":"(<code>datetime</code>)           \u2013            <p>The time the request was made.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(request_headers)","title":"<code>request_headers</code>","text":"(<code>dict</code>)           \u2013            <p>The headers sent with the request.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(request_params)","title":"<code>request_params</code>","text":"(<code>dict</code>)           \u2013            <p>The parameters sent with the request.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.add_request_record(response)","title":"<code>response</code>","text":"(<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>The response received from the request.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.disable_auditing","title":"disable_auditing","text":"<pre><code>disable_auditing() -&gt; None\n</code></pre> <p>Disables auditing by setting the enabled flag to False.</p> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul>"},{"location":"reference/#kapipy.audit_manager.AuditManager.enable_auditing","title":"enable_auditing","text":"<pre><code>enable_auditing(\n    folder: str, retain_data: bool = True\n) -&gt; None\n</code></pre> <p>Enable auditing and create the audit database if it does not exist.</p> <p>Parameters:</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.enable_auditing(folder)","title":"<code>folder</code>","text":"(<code>str</code>)           \u2013            <p>The directory where the audit database will be stored.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.enable_auditing(retain_data)","title":"<code>retain_data</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to retain audit data. Defaults to True.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.get_latest_request_for_item","title":"get_latest_request_for_item","text":"<pre><code>get_latest_request_for_item(\n    item_id: int, request_type: str = None\n) -&gt; dict\n</code></pre> <p>Returns the most recent audit record for the given item_id, optionally filtered by request_type, based on request_time.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The most recent audit record as a dictionary, or empty dictionary.</p> </li> </ul>"},{"location":"reference/#kapipy.audit_manager.AuditManager.get_latest_request_for_item(item_id)","title":"<code>item_id</code>","text":"(<code>int</code>)           \u2013            <p>The ID of the item to search for.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.get_latest_request_for_item(request_type)","title":"<code>request_type</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The type of request to filter by.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.save_data","title":"save_data","text":"<pre><code>save_data(\n    item_id: int,\n    request_type: str,\n    request_time: datetime,\n    data: dict,\n) -&gt; None\n</code></pre> <p>Save the audit data to a local JSON file.</p> <p>The file will be saved in a 'data' subfolder within the audit folder, with a filename formatted as '{request_type}{item_id}{request_time}.json'.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>             \u2013            <p>If the file or directory cannot be created or written.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the data cannot be serialized to JSON.</p> </li> </ul>"},{"location":"reference/#kapipy.audit_manager.AuditManager.save_data(item_id)","title":"<code>item_id</code>","text":"(<code>int</code>)           \u2013            <p>The ID of the item related to the data.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.save_data(request_type)","title":"<code>request_type</code>","text":"(<code>str</code>)           \u2013            <p>The type of request (e.g., 'GET', 'POST').</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.save_data(request_time)","title":"<code>request_time</code>","text":"(<code>str</code>)           \u2013            <p>The time the request was made, used in the filename.</p>"},{"location":"reference/#kapipy.audit_manager.AuditManager.save_data(data)","title":"<code>data</code>","text":"(<code>dict</code>)           \u2013            <p>The data to be saved as JSON.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem","title":"BaseItem  <code>dataclass</code>","text":"<pre><code>BaseItem(\n    id: int,\n    url: str,\n    type_: str,\n    title: str,\n    description: str,\n    data: ItemData,\n    services: str,\n    kind: str,\n    categories: List[Any],\n    tags: List[str],\n    created_at: str,\n    license: Any,\n    metadata: Any,\n    num_views: int,\n    num_downloads: int,\n)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for Items. Should not be created directly. Instead, use the ContentManager to return an Item.</p> <p>Methods:</p> <ul> <li> <code>attach_resources</code>             \u2013              <p>Attaches session, audit, and content manager resources to the item.</p> </li> <li> <code>export</code>             \u2013              <p>Exports the item in the specified format.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>feature_class_name</code>               (<code>str</code>)           \u2013            <p>Return the feature class name that would be used in an export to file geodatabase request.</p> </li> <li> <code>fgb_name</code>               (<code>str</code>)           \u2013            <p>Return the file geodatabase name that would be used in an export to file geodatabase request.</p> </li> <li> <code>supports_changesets</code>               (<code>bool</code>)           \u2013            <p>Returns whether the item supports changesets.</p> </li> </ul>"},{"location":"reference/#kapipy.data_classes.BaseItem.feature_class_name","title":"feature_class_name  <code>property</code>","text":"<pre><code>feature_class_name: str\n</code></pre> <p>Return the feature class name that would be used in an export to file geodatabase request.</p> <p>Replace any non-alphanumeric characters with underscore This seems to be the Koordinates method for setting the feature class names</p> <p>NOTE: This logic is observed from running exports only and does not appear to be documented anywhere by Koordinates.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.fgb_name","title":"fgb_name  <code>property</code>","text":"<pre><code>fgb_name: str\n</code></pre> <p>Return the file geodatabase name that would be used in an export to file geodatabase request.</p> <p>NOTE: This logic is observed from running exports only and does not appear to be documented anywhere by Koordinates.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.supports_changesets","title":"supports_changesets  <code>property</code>","text":"<pre><code>supports_changesets: bool\n</code></pre> <p>Returns whether the item supports changesets.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the item supports changesets, False otherwise.</p> </li> </ul>"},{"location":"reference/#kapipy.data_classes.BaseItem.attach_resources","title":"attach_resources","text":"<pre><code>attach_resources(\n    session: SessionManager = None,\n    audit: AuditManager = None,\n    content: ContentManager = None,\n)\n</code></pre> <p>Attaches session, audit, and content manager resources to the item.</p> <p>Parameters:</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.attach_resources(session)","title":"<code>session</code>","text":"(<code>SessionManager</code>, default:                   <code>None</code> )           \u2013            <p>The session manager to attach.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.attach_resources(audit)","title":"<code>audit</code>","text":"(<code>AuditManager</code>, default:                   <code>None</code> )           \u2013            <p>The audit manager to attach.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.attach_resources(content)","title":"<code>content</code>","text":"(<code>ContentManager</code>, default:                   <code>None</code> )           \u2013            <p>The content manager to attach.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.export","title":"export","text":"<pre><code>export(\n    export_format: str,\n    out_sr: int = None,\n    bbox_geometry: Union[GeoDataFrame, DataFrame] = None,\n    filter_geometry: Optional[\n        Union[dict, GeoDataFrame, DataFrame]\n    ] = None,\n    poll_interval: int = None,\n    timeout: int = None,\n    **kwargs: Any,\n) -&gt; JobResult\n</code></pre> <p>Exports the item in the specified format.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>JobResult</code> (              <code>JobResult</code> )          \u2013            <p>A JobResult instance containing the export job details.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If export validation fails.</p> </li> </ul>"},{"location":"reference/#kapipy.data_classes.BaseItem.export(export_format)","title":"<code>export_format</code>","text":"(<code>str</code>)           \u2013            <p>The format to export the item in.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.export(out_sr)","title":"<code>out_sr</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The coordinate reference system code to use for the export.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.export(filter_geometry)","title":"<code>filter_geometry</code>","text":"(<code>dict or GeoDataFrame or DataFrame</code>, default:                   <code>None</code> )           \u2013            <p>The filter_geometry to use for the export. Should be a GeoJSON dictionary, GeoDataFrame, or SEDF.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.export(poll_interval)","title":"<code>poll_interval</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The interval in seconds to poll the export job status. Default is 10 seconds.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.export(timeout)","title":"<code>timeout</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time in seconds to wait for the export job to complete.</p>"},{"location":"reference/#kapipy.data_classes.BaseItem.export(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the export request.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem","title":"VectorItem  <code>dataclass</code>","text":"<pre><code>VectorItem(\n    id: int,\n    url: str,\n    type_: str,\n    title: str,\n    description: str,\n    data: VectorItemData,\n    services: str,\n    kind: str,\n    categories: List[Any],\n    tags: List[str],\n    created_at: str,\n    license: Any,\n    metadata: Any,\n    num_views: int,\n    num_downloads: int,\n)\n</code></pre> <p>               Bases: <code>BaseItem</code></p> <p>Represents a vector item in the GISK content system.</p> <p>Inherits from BaseItem and provides methods for querying and retrieving changesets via WFS.</p> <p>Methods:</p> <ul> <li> <code>attach_resources</code>             \u2013              <p>Attaches session, audit, and content manager resources to the item.</p> </li> <li> <code>changeset</code>             \u2013              <p>Retrieves a changeset for the item in JSON format.</p> </li> <li> <code>export</code>             \u2013              <p>Exports the item in the specified format.</p> </li> <li> <code>query</code>             \u2013              <p>Executes a WFS query on the item and returns the result as JSON.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>feature_class_name</code>               (<code>str</code>)           \u2013            <p>Return the feature class name that would be used in an export to file geodatabase request.</p> </li> <li> <code>fgb_name</code>               (<code>str</code>)           \u2013            <p>Return the file geodatabase name that would be used in an export to file geodatabase request.</p> </li> <li> <code>supports_changesets</code>               (<code>bool</code>)           \u2013            <p>Returns whether the item supports changesets.</p> </li> </ul>"},{"location":"reference/#kapipy.vector_item.VectorItem.feature_class_name","title":"feature_class_name  <code>property</code>","text":"<pre><code>feature_class_name: str\n</code></pre> <p>Return the feature class name that would be used in an export to file geodatabase request.</p> <p>Replace any non-alphanumeric characters with underscore This seems to be the Koordinates method for setting the feature class names</p> <p>NOTE: This logic is observed from running exports only and does not appear to be documented anywhere by Koordinates.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.fgb_name","title":"fgb_name  <code>property</code>","text":"<pre><code>fgb_name: str\n</code></pre> <p>Return the file geodatabase name that would be used in an export to file geodatabase request.</p> <p>NOTE: This logic is observed from running exports only and does not appear to be documented anywhere by Koordinates.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.supports_changesets","title":"supports_changesets  <code>property</code>","text":"<pre><code>supports_changesets: bool\n</code></pre> <p>Returns whether the item supports changesets.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the item supports changesets, False otherwise.</p> </li> </ul>"},{"location":"reference/#kapipy.vector_item.VectorItem.attach_resources","title":"attach_resources","text":"<pre><code>attach_resources(\n    session: SessionManager = None,\n    audit: AuditManager = None,\n    content: ContentManager = None,\n)\n</code></pre> <p>Attaches session, audit, and content manager resources to the item.</p> <p>Parameters:</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.attach_resources(session)","title":"<code>session</code>","text":"(<code>SessionManager</code>, default:                   <code>None</code> )           \u2013            <p>The session manager to attach.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.attach_resources(audit)","title":"<code>audit</code>","text":"(<code>AuditManager</code>, default:                   <code>None</code> )           \u2013            <p>The audit manager to attach.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.attach_resources(content)","title":"<code>content</code>","text":"(<code>ContentManager</code>, default:                   <code>None</code> )           \u2013            <p>The content manager to attach.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.changeset","title":"changeset","text":"<pre><code>changeset(\n    from_time: str,\n    to_time: str = None,\n    out_sr=None,\n    cql_filter: str = None,\n    out_fields: str | list[str] = None,\n    result_record_count: int = None,\n    bbox: str = None,\n    bbox_geometry: Union[GeoDataFrame, DataFrame] = None,\n    filter_geometry: Union[GeoDataFrame, DataFrame] = None,\n    spatial_rel: str = None,\n    **kwargs: Any,\n) -&gt; dict\n</code></pre> <p>Retrieves a changeset for the item in JSON format.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The changeset data in JSON format.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item does not support changesets.</p> </li> </ul>"},{"location":"reference/#kapipy.vector_item.VectorItem.changeset(from_time)","title":"<code>from_time</code>","text":"(<code>str</code>)           \u2013            <p>The start time for the changeset query, ISO format (e.g., \"2015-05-15T04:25:25.334974\").</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.changeset(to_time)","title":"<code>to_time</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end time for the changeset query, ISO format. If not provided, the current time is used.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.changeset(cql_filter)","title":"<code>cql_filter</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The CQL filter to apply to the changeset query.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.changeset(bbox)","title":"<code>bbox</code>","text":"(<code>str or GeoDataFrame</code>, default:                   <code>None</code> )           \u2013            <p>The bounding box to apply to the changeset query. If a GeoDataFrame is provided, it will be converted to a bounding box string in WGS84.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.changeset(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the WFS query.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.export","title":"export","text":"<pre><code>export(\n    export_format: str,\n    out_sr: int = None,\n    bbox_geometry: Union[GeoDataFrame, DataFrame] = None,\n    filter_geometry: Optional[\n        Union[dict, GeoDataFrame, DataFrame]\n    ] = None,\n    poll_interval: int = None,\n    timeout: int = None,\n    **kwargs: Any,\n) -&gt; JobResult\n</code></pre> <p>Exports the item in the specified format.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>JobResult</code> (              <code>JobResult</code> )          \u2013            <p>A JobResult instance containing the export job details.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If export validation fails.</p> </li> </ul>"},{"location":"reference/#kapipy.vector_item.VectorItem.export(export_format)","title":"<code>export_format</code>","text":"(<code>str</code>)           \u2013            <p>The format to export the item in.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.export(out_sr)","title":"<code>out_sr</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The coordinate reference system code to use for the export.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.export(filter_geometry)","title":"<code>filter_geometry</code>","text":"(<code>dict or GeoDataFrame or DataFrame</code>, default:                   <code>None</code> )           \u2013            <p>The filter_geometry to use for the export. Should be a GeoJSON dictionary, GeoDataFrame, or SEDF.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.export(poll_interval)","title":"<code>poll_interval</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The interval in seconds to poll the export job status. Default is 10 seconds.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.export(timeout)","title":"<code>timeout</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time in seconds to wait for the export job to complete.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.export(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the export request.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query","title":"query","text":"<pre><code>query(\n    cql_filter: str = None,\n    out_sr: int = None,\n    out_fields: str | list[str] = None,\n    result_record_count: int = None,\n    bbox: str = None,\n    bbox_geometry: Union[GeoDataFrame, DataFrame] = None,\n    filter_geometry: Union[GeoDataFrame, DataFrame] = None,\n    spatial_rel: str = None,\n    **kwargs: Any,\n) -&gt; dict\n</code></pre> <p>Executes a WFS query on the item and returns the result as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The result of the WFS query in JSON format.</p> </li> </ul>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(cql_filter)","title":"<code>cql_filter</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The CQL filter to apply to the query.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(out_sr)","title":"<code>out_sr</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The spatial reference system code to use for the query.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(out_fields)","title":"<code>out_fields</code>","text":"(<code>str, list of strings</code>, default:                   <code>None</code> )           \u2013            <p>Attribute fields to include in the response.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(result_record_count)","title":"<code>result_record_count</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Restricts the maximum number of results to return.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(bbox)","title":"<code>bbox</code>","text":"(<code>str or GeoDataFrame or DataFrame</code>, default:                   <code>None</code> )           \u2013            <p>The bounding box to apply to the query. If a GeoDataFrame or SEDF is provided, it will be converted to a bounding box string in WGS84.</p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(bbox_geometry)","title":"<code>bbox_geometry</code>","text":"(<code>gdf or sdf</code>, default:                   <code>None</code> )           \u2013            <p>A dataframe that is converted to a bounding box and used to spatially filter the response.  </p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(filter_geometry)","title":"<code>filter_geometry</code>","text":"(<code>gdf or sdf</code>, default:                   <code>None</code> )           \u2013            <p>A dataframe that is used to spatially filter the response.  </p>"},{"location":"reference/#kapipy.vector_item.VectorItem.query(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the WFS query.</p>"},{"location":"reference/#kapipy.table_item.TableItem","title":"TableItem  <code>dataclass</code>","text":"<pre><code>TableItem(\n    id: int,\n    url: str,\n    type_: str,\n    title: str,\n    description: str,\n    data: ItemData,\n    services: str,\n    kind: str,\n    categories: List[Any],\n    tags: List[str],\n    created_at: str,\n    license: Any,\n    metadata: Any,\n    num_views: int,\n    num_downloads: int,\n)\n</code></pre> <p>               Bases: <code>BaseItem</code></p> <p>Represents a table item in the GISK content system.</p> <p>Inherits from BaseItem and provides methods for querying and retrieving changesets via WFS.</p> <p>Methods:</p> <ul> <li> <code>attach_resources</code>             \u2013              <p>Attaches session, audit, and content manager resources to the item.</p> </li> <li> <code>changeset</code>             \u2013              <p>Retrieves a changeset for the item in JSON format.</p> </li> <li> <code>export</code>             \u2013              <p>Exports the item in the specified format.</p> </li> <li> <code>query</code>             \u2013              <p>Executes a WFS query on the item and returns the result as JSON.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>feature_class_name</code>               (<code>str</code>)           \u2013            <p>Return the feature class name that would be used in an export to file geodatabase request.</p> </li> <li> <code>fgb_name</code>               (<code>str</code>)           \u2013            <p>Return the file geodatabase name that would be used in an export to file geodatabase request.</p> </li> <li> <code>supports_changesets</code>               (<code>bool</code>)           \u2013            <p>Returns whether the item supports changesets.</p> </li> </ul>"},{"location":"reference/#kapipy.table_item.TableItem.feature_class_name","title":"feature_class_name  <code>property</code>","text":"<pre><code>feature_class_name: str\n</code></pre> <p>Return the feature class name that would be used in an export to file geodatabase request.</p> <p>Replace any non-alphanumeric characters with underscore This seems to be the Koordinates method for setting the feature class names</p> <p>NOTE: This logic is observed from running exports only and does not appear to be documented anywhere by Koordinates.</p>"},{"location":"reference/#kapipy.table_item.TableItem.fgb_name","title":"fgb_name  <code>property</code>","text":"<pre><code>fgb_name: str\n</code></pre> <p>Return the file geodatabase name that would be used in an export to file geodatabase request.</p> <p>NOTE: This logic is observed from running exports only and does not appear to be documented anywhere by Koordinates.</p>"},{"location":"reference/#kapipy.table_item.TableItem.supports_changesets","title":"supports_changesets  <code>property</code>","text":"<pre><code>supports_changesets: bool\n</code></pre> <p>Returns whether the item supports changesets.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the item supports changesets, False otherwise.</p> </li> </ul>"},{"location":"reference/#kapipy.table_item.TableItem.attach_resources","title":"attach_resources","text":"<pre><code>attach_resources(\n    session: SessionManager = None,\n    audit: AuditManager = None,\n    content: ContentManager = None,\n)\n</code></pre> <p>Attaches session, audit, and content manager resources to the item.</p> <p>Parameters:</p>"},{"location":"reference/#kapipy.table_item.TableItem.attach_resources(session)","title":"<code>session</code>","text":"(<code>SessionManager</code>, default:                   <code>None</code> )           \u2013            <p>The session manager to attach.</p>"},{"location":"reference/#kapipy.table_item.TableItem.attach_resources(audit)","title":"<code>audit</code>","text":"(<code>AuditManager</code>, default:                   <code>None</code> )           \u2013            <p>The audit manager to attach.</p>"},{"location":"reference/#kapipy.table_item.TableItem.attach_resources(content)","title":"<code>content</code>","text":"(<code>ContentManager</code>, default:                   <code>None</code> )           \u2013            <p>The content manager to attach.</p>"},{"location":"reference/#kapipy.table_item.TableItem.changeset","title":"changeset","text":"<pre><code>changeset(\n    from_time: str,\n    to_time: str = None,\n    cql_filter: str = None,\n    **kwargs: Any,\n) -&gt; dict\n</code></pre> <p>Retrieves a changeset for the item in JSON format.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The changeset data in JSON format.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the item does not support changesets.</p> </li> </ul>"},{"location":"reference/#kapipy.table_item.TableItem.changeset(from_time)","title":"<code>from_time</code>","text":"(<code>str</code>)           \u2013            <p>The start time for the changeset query, ISO format (e.g., \"2015-05-15T04:25:25.334974\").</p>"},{"location":"reference/#kapipy.table_item.TableItem.changeset(to_time)","title":"<code>to_time</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end time for the changeset query, ISO format. If not provided, the current time is used.</p>"},{"location":"reference/#kapipy.table_item.TableItem.changeset(cql_filter)","title":"<code>cql_filter</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The CQL filter to apply to the changeset query.</p>"},{"location":"reference/#kapipy.table_item.TableItem.changeset(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the WFS query.</p>"},{"location":"reference/#kapipy.table_item.TableItem.export","title":"export","text":"<pre><code>export(\n    export_format: str,\n    out_sr: int = None,\n    bbox_geometry: Union[GeoDataFrame, DataFrame] = None,\n    filter_geometry: Optional[\n        Union[dict, GeoDataFrame, DataFrame]\n    ] = None,\n    poll_interval: int = None,\n    timeout: int = None,\n    **kwargs: Any,\n) -&gt; JobResult\n</code></pre> <p>Exports the item in the specified format.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>JobResult</code> (              <code>JobResult</code> )          \u2013            <p>A JobResult instance containing the export job details.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If export validation fails.</p> </li> </ul>"},{"location":"reference/#kapipy.table_item.TableItem.export(export_format)","title":"<code>export_format</code>","text":"(<code>str</code>)           \u2013            <p>The format to export the item in.</p>"},{"location":"reference/#kapipy.table_item.TableItem.export(out_sr)","title":"<code>out_sr</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The coordinate reference system code to use for the export.</p>"},{"location":"reference/#kapipy.table_item.TableItem.export(filter_geometry)","title":"<code>filter_geometry</code>","text":"(<code>dict or GeoDataFrame or DataFrame</code>, default:                   <code>None</code> )           \u2013            <p>The filter_geometry to use for the export. Should be a GeoJSON dictionary, GeoDataFrame, or SEDF.</p>"},{"location":"reference/#kapipy.table_item.TableItem.export(poll_interval)","title":"<code>poll_interval</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The interval in seconds to poll the export job status. Default is 10 seconds.</p>"},{"location":"reference/#kapipy.table_item.TableItem.export(timeout)","title":"<code>timeout</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The maximum time in seconds to wait for the export job to complete.</p>"},{"location":"reference/#kapipy.table_item.TableItem.export(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the export request.</p>"},{"location":"reference/#kapipy.table_item.TableItem.query","title":"query","text":"<pre><code>query(cql_filter: str = None, **kwargs: Any) -&gt; dict\n</code></pre> <p>Executes a WFS query on the item and returns the result as JSON.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The result of the WFS query in JSON format.</p> </li> </ul>"},{"location":"reference/#kapipy.table_item.TableItem.query(cql_filter)","title":"<code>cql_filter</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The CQL filter to apply to the query.</p>"},{"location":"reference/#kapipy.table_item.TableItem.query(**kwargs)","title":"<code>**kwargs</code>","text":"(<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters for the WFS query.</p>"},{"location":"reference/#kapipy.data_classes.ItemData","title":"ItemData  <code>dataclass</code>","text":"<pre><code>ItemData(\n    storage: Optional[str],\n    datasources: Optional[str],\n    fields: List[FieldDef],\n    encoding: Optional[str],\n    primary_key_fields: Optional[List[str]],\n    source_revision: Optional[int],\n    omitted_fields: List[Any],\n    tile_revision: int,\n    feature_count: int,\n    datasource_count: int,\n    change_summary: Optional[ChangeSummary],\n    source_summary: Optional[str],\n    import_started_at: str,\n    import_ended_at: str,\n    import_log: ImportLog,\n    import_version: str,\n    update_available: bool,\n    sample: Optional[str],\n    raster_resolution: Optional[Any],\n    empty_geometry_count: int,\n    has_z: bool,\n    export_formats: List[ExportFormat],\n)\n</code></pre>"},{"location":"reference/#kapipy.data_classes.VectorItemData","title":"VectorItemData  <code>dataclass</code>","text":"<pre><code>VectorItemData(\n    storage: Optional[str],\n    datasources: Optional[str],\n    fields: List[FieldDef],\n    encoding: Optional[str],\n    primary_key_fields: Optional[List[str]],\n    source_revision: Optional[int],\n    omitted_fields: List[Any],\n    tile_revision: int,\n    feature_count: int,\n    datasource_count: int,\n    change_summary: Optional[ChangeSummary],\n    source_summary: Optional[str],\n    import_started_at: str,\n    import_ended_at: str,\n    import_log: ImportLog,\n    import_version: str,\n    update_available: bool,\n    sample: Optional[str],\n    raster_resolution: Optional[Any],\n    empty_geometry_count: int,\n    has_z: bool,\n    export_formats: List[ExportFormat],\n    crs: CRS,\n    geometry_field: str,\n    geometry_type: str,\n    extent: Dict[str, Any],\n)\n</code></pre> <p>               Bases: <code>ItemData</code></p>"},{"location":"reference/#kapipy.wfs_response.WFSResponse","title":"WFSResponse","text":"<pre><code>WFSResponse(\n    geojson: dict, item: BaseItem = None, out_sr=None\n)\n</code></pre> <p>Represents a response from a WFS (Web Feature Service) request.</p> <p>Holds the raw GeoJSON data and provides properties and methods to convert the data into various dataframe formats (Pandas, GeoPandas, Spatially Enabled DataFrame).</p> <p>Attributes:</p> <ul> <li> <code>_json</code>               (<code>dict</code>)           \u2013            <p>The raw GeoJSON data.</p> </li> <li> <code>item</code>               (<code>BaseItem</code>)           \u2013            <p>The associated item metadata.</p> </li> <li> <code>out_sr</code>               (<code>Any</code>)           \u2013            <p>The output spatial reference.</p> </li> <li> <code>_df</code>               (<code>DataFrame or None</code>)           \u2013            <p>Cached Pandas DataFrame.</p> </li> <li> <code>_gdf</code>               (<code>GeoDataFrame or None</code>)           \u2013            <p>Cached GeoPandas DataFrame.</p> </li> <li> <code>_sdf</code>               (<code>SpatialDataFrame or None</code>)           \u2013            <p>Cached Spatially Enabled DataFrame.</p> </li> <li> <code>total_features</code>               (<code>int</code>)           \u2013            <p>The number of features in the GeoJSON.</p> </li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"reference/#kapipy.wfs_response.WFSResponse(geojson)","title":"<code>geojson</code>","text":"(<code>dict</code>)           \u2013            <p>The raw GeoJSON data.</p>"},{"location":"reference/#kapipy.wfs_response.WFSResponse(item)","title":"<code>item</code>","text":"(<code>BaseItem</code>, default:                   <code>None</code> )           \u2013            <p>The associated item metadata.</p>"},{"location":"reference/#kapipy.wfs_response.WFSResponse(out_sr)","title":"<code>out_sr</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>The output spatial reference.</p>"},{"location":"reference/#kapipy.wfs_response.WFSResponse.df","title":"df  <code>property</code>","text":"<pre><code>df: DataFrame\n</code></pre> <p>Convert the GeoJSON to a Pandas DataFrame.</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: The features as a Pandas DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>Exception</code>             \u2013            <p>If conversion fails.</p> </li> </ul>"},{"location":"reference/#kapipy.wfs_response.WFSResponse.gdf","title":"gdf  <code>property</code>","text":"<pre><code>gdf: GeoDataFrame\n</code></pre> <p>Convert the GeoJSON to a GeoPandas DataFrame.</p> <p>Requires the geopandas package to be installed.</p> <p>Returns:</p> <ul> <li> <code>GeoDataFrame</code>           \u2013            <p>gpd.GeoDataFrame: The features as a GeoPandas DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the geopandas package is not installed.</p> </li> <li> <code>Exception</code>             \u2013            <p>If conversion fails.</p> </li> </ul>"},{"location":"reference/#kapipy.wfs_response.WFSResponse.json","title":"json  <code>property</code>","text":"<pre><code>json: dict\n</code></pre> <p>Get the raw GeoJSON data.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The raw GeoJSON data.</p> </li> </ul>"},{"location":"reference/#kapipy.wfs_response.WFSResponse.sdf","title":"sdf  <code>property</code>","text":"<pre><code>sdf: DataFrame\n</code></pre> <p>Convert the GeoJSON to a Spatially Enabled DataFrame (ArcGIS SEDF).</p> <p>Requires the arcgis package to be installed.</p> <p>Returns:</p> <ul> <li> <code>SpatialDataFrame</code> (              <code>DataFrame</code> )          \u2013            <p>The features as a Spatially Enabled DataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the arcgis package is not installed.</p> </li> <li> <code>Exception</code>             \u2013            <p>If conversion fails.</p> </li> </ul>"},{"location":"reference/#kapipy.data_classes.ExportFormat","title":"ExportFormat  <code>dataclass</code>","text":"<pre><code>ExportFormat(name: str, mimetype: str)\n</code></pre>"},{"location":"reference/#kapipy.job_result.JobResult","title":"JobResult","text":"<pre><code>JobResult(\n    payload: dict,\n    session: SessionManager,\n    poll_interval: int = None,\n    timeout: int = None,\n)\n</code></pre> <p>Represents the result of an asynchronous export or processing job.</p> <p>Provides methods to poll for job completion, retrieve job status, and download results. The download and download_async methods return a DownloadResult object containing detailed metadata about the downloaded file. Download metadata is also stored as attributes on the JobResult instance after a successful download.</p> <p>Attributes:</p> <ul> <li> <code>_initial_payload</code>               (<code>dict</code>)           \u2013            <p>The initial job payload from the API.</p> </li> <li> <code>_job_url</code>               (<code>str</code>)           \u2013            <p>The URL to poll for job status.</p> </li> <li> <code>_id</code>               (<code>int</code>)           \u2013            <p>The unique identifier of the job.</p> </li> <li> <code>_poll_interval</code>               (<code>int</code>)           \u2013            <p>Polling interval in seconds.</p> </li> <li> <code>_timeout</code>               (<code>int</code>)           \u2013            <p>Maximum time to wait for job completion in seconds.</p> </li> <li> <code>_last_response</code>               (<code>dict</code>)           \u2013            <p>The most recent job status response.</p> </li> <li> <code>#</code>               (<code>Populated after download</code>)           \u2013            </li> <li> <code>download_folder</code>               (<code>str</code>)           \u2013            <p>The directory where the file was saved.</p> </li> <li> <code>download_filename</code>               (<code>str</code>)           \u2013            <p>The name of the downloaded file.</p> </li> <li> <code>download_file_path</code>               (<code>str</code>)           \u2013            <p>The full path to the downloaded file.</p> </li> <li> <code>download_file_size_bytes</code>               (<code>int</code>)           \u2013            <p>The size of the downloaded file in bytes.</p> </li> <li> <code>download_completed_at</code>               (<code>float</code>)           \u2013            <p>The timestamp when the download completed.</p> </li> <li> <code>download_resolved_url</code>               (<code>str</code>)           \u2013            <p>The final resolved URL after redirects.</p> </li> <li> <code>download_checksum</code>               (<code>str | None</code>)           \u2013            <p>The SHA256 checksum of the downloaded file.</p> </li> </ul> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>download</code>             \u2013              <p>Waits for the job to finish, then downloads the file synchronously.</p> </li> <li> <code>output</code>             \u2013              <p>Blocks until the job completes, then returns the final job response.</p> </li> <li> <code>to_dict</code>             \u2013              <p>Returns the most recent job status response as a dictionary.</p> </li> </ul>"},{"location":"reference/#kapipy.job_result.JobResult(payload)","title":"<code>payload</code>","text":"(<code>dict</code>)           \u2013            <p>The job payload, typically from an API response.</p>"},{"location":"reference/#kapipy.job_result.JobResult(session)","title":"<code>session</code>","text":"(<code>SessionManager</code>)           \u2013            <p>The GISK SessionManager.</p>"},{"location":"reference/#kapipy.job_result.JobResult(poll_interval)","title":"<code>poll_interval</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Interval in seconds to poll the job status. Default is 10.</p>"},{"location":"reference/#kapipy.job_result.JobResult(timeout)","title":"<code>timeout</code>","text":"(<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Maximum time in seconds to wait for the job to complete. Default is 1800 (30 min).</p>"},{"location":"reference/#kapipy.job_result.JobResult.created_at","title":"created_at  <code>property</code>","text":"<pre><code>created_at: str | None\n</code></pre> <p>Returns the creation time of the job.</p> <p>Returns:</p> <ul> <li> <code>str | None</code>           \u2013            <p>str | None: The creation timestamp, or None if not available.</p> </li> </ul>"},{"location":"reference/#kapipy.job_result.JobResult.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the name of the job.</p>"},{"location":"reference/#kapipy.job_result.JobResult.progress","title":"progress  <code>property</code>","text":"<pre><code>progress: float | None\n</code></pre> <p>Returns the progress of the job as a percentage.</p> <p>Returns:</p> <ul> <li> <code>float | None</code>           \u2013            <p>float | None: The progress value, or None if not available.</p> </li> </ul>"},{"location":"reference/#kapipy.job_result.JobResult.state","title":"state  <code>property</code>","text":"<pre><code>state: str\n</code></pre> <p>Returns the current state of the job.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The job state. Possible values include 'complete', 'processing', 'cancelled', 'error', 'gone'.</p> </li> </ul>"},{"location":"reference/#kapipy.job_result.JobResult.status","title":"status  <code>property</code>","text":"<pre><code>status: JobStatus\n</code></pre> <p>Refreshes and returns the current job status.</p> <p>Returns:</p> <ul> <li> <code>JobStatus</code> (              <code>JobStatus</code> )          \u2013            <p>The state and progress of the job.</p> </li> </ul>"},{"location":"reference/#kapipy.job_result.JobResult.download","title":"download","text":"<pre><code>download(\n    folder: str, file_name: str | None = None\n) -&gt; DownloadResult\n</code></pre> <p>Waits for the job to finish, then downloads the file synchronously.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DownloadResult</code> (              <code>DownloadResult</code> )          \u2013            <p>Object containing details about the downloaded file.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the download URL is not available.</p> </li> </ul>"},{"location":"reference/#kapipy.job_result.JobResult.download(folder)","title":"<code>folder</code>","text":"(<code>str</code>)           \u2013            <p>The folder where the file will be saved.</p>"},{"location":"reference/#kapipy.job_result.JobResult.download(file_name)","title":"<code>file_name</code>","text":"(<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The name of the file to save. If None, uses the job name.</p>"},{"location":"reference/#kapipy.job_result.JobResult.output","title":"output","text":"<pre><code>output() -&gt; dict\n</code></pre> <p>Blocks until the job completes, then returns the final job response.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The final job response after completion.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TimeoutError</code>             \u2013            <p>If the job does not complete within the timeout.</p> </li> <li> <code>RuntimeError</code>             \u2013            <p>If the job fails or is cancelled.</p> </li> </ul>"},{"location":"reference/#kapipy.job_result.JobResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Returns the most recent job status response as a dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>The most recent job status response.</p> </li> </ul>"},{"location":"reference/#custom-errors","title":"Custom Errors","text":""},{"location":"reference/#kapipy.custom_errors","title":"custom_errors","text":"<p>custom_errors.py Custom exceptions.</p> <p>Classes:</p> <ul> <li> <code>BadRequest</code>           \u2013            <p>Exception raised for HTTP 400 Bad Request errors.</p> </li> <li> <code>ExportError</code>           \u2013            <p>Custom exception for errors encountered during export operations.</p> </li> <li> <code>HTTPError</code>           \u2013            <p>Base exception for HTTP errors.</p> </li> <li> <code>NotFound</code>           \u2013            <p>Exception raised for HTTP 404 Not Found errors.</p> </li> <li> <code>ServerError</code>           \u2013            <p>Exception raised for HTTP 500 Server Error errors.</p> </li> <li> <code>Unauthorized</code>           \u2013            <p>Exception raised for HTTP 401 Unauthorized errors.</p> </li> <li> <code>UnknownItemTypeError</code>           \u2013            <p>Exception raised when an unknown item type is encountered.</p> </li> </ul>"},{"location":"reference/#kapipy.custom_errors.BadRequest","title":"BadRequest","text":"<p>               Bases: <code>HTTPError</code></p> <p>Exception raised for HTTP 400 Bad Request errors.</p>"},{"location":"reference/#kapipy.custom_errors.ExportError","title":"ExportError","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for errors encountered during export operations.</p>"},{"location":"reference/#kapipy.custom_errors.HTTPError","title":"HTTPError","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for HTTP errors.</p>"},{"location":"reference/#kapipy.custom_errors.NotFound","title":"NotFound","text":"<p>               Bases: <code>HTTPError</code></p> <p>Exception raised for HTTP 404 Not Found errors.</p>"},{"location":"reference/#kapipy.custom_errors.ServerError","title":"ServerError","text":"<p>               Bases: <code>HTTPError</code></p> <p>Exception raised for HTTP 500 Server Error errors.</p>"},{"location":"reference/#kapipy.custom_errors.Unauthorized","title":"Unauthorized","text":"<p>               Bases: <code>HTTPError</code></p> <p>Exception raised for HTTP 401 Unauthorized errors.</p>"},{"location":"reference/#kapipy.custom_errors.UnknownItemTypeError","title":"UnknownItemTypeError","text":"<pre><code>UnknownItemTypeError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Exception raised when an unknown item type is encountered.</p> <p>This exception is used to signal that an item kind is not supported by the client.</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>message</code>               (<code>str</code>)           \u2013            <p>Description of the error.</p> </li> </ul> <p>Parameters:</p> <ul> <li> </li> </ul>"},{"location":"reference/#kapipy.custom_errors.UnknownItemTypeError(message)","title":"<code>message</code>","text":"(<code>str</code>)           \u2013            <p>Description of the error.</p>"},{"location":"reference/#kapipy.custom_errors.UnknownItemTypeError(message)","title":"<code>message</code>","text":"(<code>str</code>)           \u2013            <p>Description of the error.</p>"},{"location":"usage/","title":"Usage Guide","text":"<p>This guide walks you through the main ways to use the <code>kapipy</code> package to query and download data from the LINZ Data Service via Koordinates.</p>"},{"location":"usage/#installation-notes","title":"Installation Notes","text":"<p>Kapipy is designed to use either GeoPandas or the ArcGIS API for Python, returning and reading data as either a GeoDataFrame or a Spatially Enabled DataFrame respectively.  Neither package is defined as a requirement of kapipy, as users may choose to use one over the other and may not want the other automatically installed.  </p> <p>This means you need to manually install one of either geopandas or arcgis into your Python environment.</p>"},{"location":"usage/#arcgis","title":"ArcGIS","text":"<p>If you are an ArcGIS user, cloning the default conda environment from ArcGIS Pro or ArcGIS Server should be sufficient, and you just need to install kapipy. If you choose to start with a blank environment and do not intend to install arcpy, you may need to install the following: - pyproj - shapely - pyshp  </p>"},{"location":"usage/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>If you are starting with a clean Python environment and want to use Jupyter Notebooks (e.g. inside Visual Studio Code), then manually install these packages: - ipykernel  </p>"},{"location":"usage/#connecting-to-the-various-open-data-portals","title":"Connecting to the various open data portals","text":"<p>Import the GISK class and then create a connection.  </p> <p>LINZ, Stats NZ and LRIS have built in names for convenience. Alternatively, pass in the base URL.  </p> <pre><code>from kapipy.gis import GISK\n\nlinz = GISK(name=\"linz\", api_key=\"your-linz-api-key\")\nstatsnz = GISK(name='statsnz', api_key=\"your-stats-api-key\")\nlris = GISK(name='lris', api_key=\"your-lris-api-key\")\n</code></pre> <p>Passing in a base url:  </p> <pre><code>linz = GISK(url=\"https://data.linz.govt.nz/\", api_key=\"your-linz-api-key\")\n</code></pre>"},{"location":"usage/#get-a-reference-to-an-item","title":"Get a reference to an item","text":"<p>The gis object has a property called content which is a ContentManager. This allows you to get a reference to an item using it's id.  </p> <pre><code>from kapipy.gis import GISK\n\n#create gis object\nlinz = GISK(name=\"linz\", api_key=\"your-linz-api-key\")\n\n#get item object\nrail_station_layer_id = \"50318\" #rail station 175 points\nitm = linz.content.get(rail_station_layer_id)\n\nprint(itm)\n</code></pre>"},{"location":"usage/#wfs-queries","title":"WFS queries","text":"<p>Items with WFS endpoints can be queried using the query and, if the item supports changesets, changeset methods of the item .</p> <p>For spatial items, it is recommended to specify a desired spatial reference via the out_sr parameter.  </p>"},{"location":"usage/#query","title":"Query","text":"<p>Get all data  </p> <pre><code>data = itm.query(out_sr=2193)\n</code></pre> <p>Get first 5 records.  </p> <pre><code>data = itm.query(result_record_count=5, out_sr=2193)\n</code></pre> <p>Use a CQL Filter query to filter by attribute information  </p> <pre><code>data = itm.query(cql_filter=\"name='Matamata Station'\", out_sr=2193)\n</code></pre> <p>Data is returned as a WFSResponse object. This has four attributes that can be used to access the data: - .json: this provides the raw json response returned from the WFS service. - .df: this provides a Pandas DataFrame of the data. - .gdf: if geopandas is installed, this will return a GeoDataFrame of the data. - .sdf: if arcgis is installed, this will return a Spatially Enabled DataFrame of the data.  </p> <p>The attribute types of the dataframes are set according to the item's field list.  </p> <pre><code>print(data.item.data.fields)\nprint(data.sdf.dtypes())\nprint(data.sdf.head())\n</code></pre> <p>Only fetch specified attribute fields. Remember to include the geometry field if you want that in the tabular data.  </p> <pre><code>data = itm.query(\n    out_fields=['id', 'geodetic_code', itm.data.geometry_field],\n    out_sr = 2193)\nprint(f\"Total records returned {itm.title}: {data.df.shape[0]}\")\ndata.df.head()\n</code></pre>"},{"location":"usage/#changeset","title":"Changeset","text":"<p>Also returned as a WFSResponse object with the same logic as the query method. The datetime parameters should be provided in ISO 8601 format.  </p> <p>The from_time parameter is the time from which the changeset data will be generated. The to_time parameter is optional, and is the time up to which the changeset data will be generated. If this parameter is not provided then it defaults to now.  </p> <pre><code>data = itm.changeset(from_time=\"2024-01-01T00:00:00Z\", out_sr=2193)\nprint((f\"Total records returned {itm.title}: {data.gdf.shape[0]}\"))\n</code></pre>"},{"location":"usage/#query-with-a-spatial-filter","title":"Query with a spatial filter","text":"<p>The filter_geometry argument can be passed in as a gdf or sdf.  </p> <p>It is recommended to only have one polygon geometry in the dataframe, and avoid complex geometries with lots of vertices. If your use case requires a complex polygon geometry that the API can't handle, consider making the query using the bbox extent and then doing a local intersect on the data afterwards to drop the features you don't want.   </p> <p>If there is more than one record in the dataframe, the records will be unioned into one geometry. The more records, the longer this will take which is why it is recommended to only have one record or at least minimise the number.    </p> <p>The following example will only return features that intersect the filter_geometry object.  </p> <pre><code>data = itm.changeset(\n    from_time=\"2024-01-01T00:00:00Z\", \n    out_sr=2193,\n    filter_geometry=matamata_gdf\n    )\nprint(f\"Total records returned {itm.title}: {data.sdf.shape[0]}\")\ndata.sdf.head()\n</code></pre>"},{"location":"usage/#query-using-a-spatial-filter-extent-bbox","title":"Query using a spatial filter extent - bbox","text":"<p>The bbox_geometry argument can be passed in as a gdf or sdf.  </p> <p>If there is more than one record in the dataframe, the extent of all geometries is calculated. The more records, the longer this will take which is why it is recommended to only have one record or at least minimise the number.     </p> <p>The following example will only return features that intersect the bounding box extent of the bbox_geometry object.  </p> <pre><code>data = itm.changeset(\n    from_time=\"2024-01-01T00:00:00Z\", \n    out_sr=2193,\n    bbox_geometry=matamata_gdf\n    )\nprint(f\"Total records returned {itm.title}: {data.sdf.shape[0]}\")\ndata.sdf.head()\n</code></pre>"},{"location":"usage/#export-data","title":"Export data","text":"<p>Exporting data creates an asynchronous task on the data portal server that returns a job id. It is possible to create and manage individual downloads, or treat them collectively.  </p> <p>Again, it is recommended to always specify the out_sr.  </p>"},{"location":"usage/#export-formats","title":"Export formats","text":"<p>You can check the available export formats by using the data.export_formats property of an item.  </p> <pre><code>print(itm.data.export_formats)\n</code></pre>"},{"location":"usage/#single-item-export","title":"Single item export","text":"<p>The item export method initiates the creation of the job on the server and a JobResult object is returned. Accessing the status property triggers a check with the server to get the latest status of the job. The status returned is a JobStatus object that has state and progress properties.  </p> <pre><code>job = itm.export(\"geodatabase\", out_sr=2193)\nprint(job.status)\n</code></pre> <p>Download the job data once it is ready. If this method is called before the job state is 'complete', it will poll the status of the job until it is ready and then downloads it. Calling the download method of a JobResult object gives you the flexibility of specifying a specific folder for that download.  </p> <pre><code>job.download(folder=r\"c:/temp\")\n</code></pre>"},{"location":"usage/#generate-an-export-using-a-spatial-filter","title":"Generate an export using a spatial filter","text":"<p>The filter_geometry argument can be passed in as a gdf, sdf or geojson.  </p> <p>It is recommended to only have one polygon geometry in the dataframe, and avoid complex geometries with lots of vertices. If there is more than one record in the dataframe, the records will be unioned into one geometry.  </p> <p>The following example will only return features that intersect the filter_geometry object.  </p> <pre><code># gdf\nmatamata_gdf = gpd.read_file(\"../examples/matamata_piako.shp\")\n# sdf\nmatamata_sdf = pd.DataFrame.spatial.from_featureclass(\"../examples/matamata_piako.shp\")\n\njob = itm.export(\"geodatabase\", out_sr=2193, filter_geometry=matamata_sdf,)\n</code></pre>"},{"location":"usage/#export-and-download-multiple-items","title":"Export and download multiple items","text":"<p>Whenever the export method of an item is called, the JobResult object is added to a list belonging to the ContentManager called jobs. </p> <p>The ContentManager has a download method as well. Calling this method and passing in a folder will download to that folder any jobs in the content manager's job list that are not already downloaded.  </p> <p>NOTE The download method of the ContentManager has no concept of a timeout period. It will just keep polling every job until they either finish or error.  </p> <pre><code>itm1.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\nitm2.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\n\nlinz.content.download(folder=r\"c:/temp\")\n</code></pre> <p>The ContentManager also has a download_folder property. You can set this and it will be used as the default if no folder is provided.  </p> <pre><code>linz.content.download_folder = r\"c:/temp\"\n\nitm1.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\nitm2.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\n\nlinz.content.download()\n</code></pre> <p>Alternatively, you can pass the content manager's download method a list of specific jobs and only those jobs will be downloaded.  </p> <pre><code>job_1 = itm1.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\njob_2 = itm2.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\njob_3 = itm3.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\njob_4 = itm4.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\njob_5 = itm5.export(\"geodatabase\", out_sr=2193, extent=matamata_sdf,)\n\n# only jobs 1, 3 and 5 will be downloaded\nlinz.content.download([job_1, job_3, job_5])\n</code></pre> <p>Once a job is downloaded, it's \"downloaded\" attribute will be set to True, and any future calls to the ContentManager's download method will not download it. Use the 'force_all' parameter to force a download of all jobs in the list, regardless of their download status.  </p> <pre><code>linz.content.download(force_all=True)\n</code></pre> <p>You can iterate over the jobs.  </p> <pre><code>for job in linz.content.jobs:\n    print(job.download_file_path)\n</code></pre>"},{"location":"usage/#audit-manager","title":"Audit Manager","text":"<p>The Audit Manager is optional. If enabled, it: - Records details of every query and export in a sqlite database. - Saves a copy of every query response as a file.  </p> <pre><code>from kapipy.gis import GISK\n\n# create gis object\nlinz = GISK(name=\"linz\", api_key=\"your-linz-api-key\")\n\n# enable audit logging\nlinz.audit.enable_auditing(folder=r\"c:/temp/audit\")\n\n# get item object\nrail_station_layer_id = \"50318\" #rail station 175 points\nitm = linz.content.get(rail_station_layer_id)\n\n# Any query will now automatically populate a record in the audit database\ndata = itm.query()\n</code></pre> <p>A copy of each response is saved as a json file. You can optionally disable this by passing in retain_data=False when enabling the audit manager.  </p> <pre><code>linz.audit.enable_auditing(folder=r\"c:/temp/audit\", retain_data=False)\n</code></pre> <p>The Audit Manager does not perform any clean up actions. You may wish to periodically purge old records from the sqlite database and/or the json files.  </p> <p>The .export() method does not record the total_features count. This is because the data is returned as a zip file in any one of several formats, and to compute the counts would require the overhead of unzipping, handling reading in any format then computing the actual counts. Doing this, for example, on the NZ Parcels layer with ~2.7 million records is non-trivial and therefore not undertaken.  </p> <p>The most recent record for a given id can be retrieved.  </p> <pre><code># returns record of the most recent query recorded.  \nlatest = linz.audit.get_latest_request_for_item(rail_station_layer_id)\n</code></pre>"}]}